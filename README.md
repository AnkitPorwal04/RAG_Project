````markdown
# ğŸ” ChatGroq RAG Chatbot

A **Retrieval-Augmented Generation (RAG)** web application powered by **Streamlit**, **LangChain**, **Groq's LLMs**, and **FAISS**, designed to allow real-time question answering from custom document sources or live websites.

Built by **Ankit Porwal**.

---

## ğŸš€ Features

- ğŸ”— Loads content directly from a live web page
- ğŸ“š Splits and embeds content using open-source **Ollama embeddings**
- ğŸ§  Stores vectorized content using **FAISS**
- âš¡ Uses **Groq's ultra-fast LLMs (like LLaMA 3)** for answering queries
- ğŸ–¥ï¸ Simple and interactive **Streamlit UI**
- ğŸ” Shows document chunks used in answering the question (document similarity search)

---

## ğŸ› ï¸ Tech Stack

| Tool           | Purpose                            |
|----------------|------------------------------------|
| Streamlit      | Frontend Web App UI                |
| LangChain      | RAG chaining and document handling |
| Groq API       | LLM inference                      |
| Ollama         | Open-source embeddings             |
| FAISS          | Vector store for similarity search |
| WebBaseLoader  | Loads data from websites           |
| Python         | Core language                      |

---

## ğŸ“¦ Installation

1. **Clone the repository:**

```bash
git clone https://github.com/your-username/chatgroq-rag.git
cd chatgroq-rag
````

2. **Install dependencies:**

```bash
pip install -r requirements.txt
```

3. **Set up environment variables:**

Create a `.env` file and add your API key:

```env
GROQ_API_KEY=your_groq_api_key
```

4. **Run the Streamlit app:**

```bash
streamlit run app.py
```

---

## ğŸ“„ Project Structure

```
chatgroq-rag/
â”‚
â”œâ”€â”€ app.py                  # Main Streamlit app
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env                    # API key storage
â””â”€â”€ README.md               # Project documentation
```

---

## ğŸ“¸ Preview

<img width="1919" height="616" alt="Screenshot 2025-07-12 223830" src="https://github.com/user-attachments/assets/98823251-4e75-44b6-b075-6477652f0b73" />

---

## ğŸ§  How It Works

1. Loads content from a provided web page (e.g., a LangChain or ML article).
2. Splits the text into chunks and creates embeddings using `OllamaEmbeddings`.
3. Stores those embeddings in a local FAISS vector store.
4. User submits a question through the Streamlit interface.
5. LangChainâ€™s retrieval chain fetches the most relevant chunks.
6. The selected LLM (via Groq API) answers based on the retrieved context.

---

## ğŸ“¬ Contact

Feel free to connect:

* ğŸ’¼ [LinkedIn â€“ Ankit Porwal](https://www.linkedin.com/in/ankitporwal04)
* ğŸ“§ [ankit.email@example.com](mailto:ankitporwal4403@gmail.com)

---

## ğŸªª License

This project is open-source and available under the [MIT License](LICENSE).

```

---

Let me know if you'd like to customize this further for:
- GitHub topics and tags
- PDF document ingestion (if you're using it later)
- Deployment (e.g., Streamlit Cloud or Hugging Face Spaces)

Just drop me a note!
```
